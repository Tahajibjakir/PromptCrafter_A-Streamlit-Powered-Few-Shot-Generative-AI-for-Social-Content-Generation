# PromptCrafter_A-Streamlit-Powered-Few-Shot-Generative-AI-for-Social-Content-Generation
PromptCrafter is a powerful and user-friendly Generative AI web application that automatically generates LinkedIn-style social media posts using few-shot learning, structured prompt engineering, and LLM-based text generation. Designed with both practical applications and academic relevance in mind, this tool demonstrates how modern LLMs can be customized for creative content generation through a clean UI and intelligent prompt chaining. Built using Streamlit as the frontend and LangChain with Groqâ€™s LLaMA 3.3-70 b-versatile as the backend LLM, the app allows users to select the desired topic, length, and language (English or Banglish) for the post. It then dynamically generates a well-structured LinkedIn post by sampling from a curated dataset of examples.

# Demo (Performance)
![Image](https://github.com/user-attachments/assets/a94be5a4-140c-441c-af25-aa9818572c62)

# Features
âœ… Streamlit UI with leftâ€“right layout for intuitive interaction and real-time preview <br/>
âœ… Few-Shot Learning powered by post filtering based on tags, length, and language <br/>
âœ… Prompt Construction Pipeline using LangChain with preloaded examples <br/>
âœ… Groq LLaMA 3 Integration for blazing fast LLM responses <br/>
âœ… Preprocessing Module to extract metadata (tags, language, line count) from raw posts <br/>
âœ… Tag Unification using an LLM to standardize tags for consistency <br/>
âœ… Multilingual Output with support for English and Banglish <br/>
âœ… Structured JSON Dataset for scalability and reproducibility <br/>
âœ… Ideal for showcasing in AI research, LLM-powered tools, or portfolio projects <br/>

# Installation

git clone https://github.com/yourusername/linkedin-post-generator.git
cd linkedin-post-generator

Create virtual env (optional)
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows

pip install -r requirements.txt

# Model Details
This project uses Groq-hosted LLaMA-3.3-70 b-versatile via the langchain_groq integration for fast and efficient generation.<br\> The LLM is guided through a few-shot prompt strategy where example posts are dynamically injected based on selected inputs like topic, length, and language.

Model Type: Generative Large Language Model (Groq LLaMA 3)

Prompt Strategy: Few-shot learning using LangChainâ€™s PromptTemplate module

Response Parsing: Handled using JsonOutputParser for structured metadata extraction (line count, tags, etc.)

Metadata Extraction: Custom LangChain prompt to extract metadata from raw posts during preprocessing

Tag Normalization: Another LLM-based prompt pipeline to unify diverse tags into a controlled set for consistency

# Project Structure

PromptCrafter/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw_post_data.json           # Raw LinkedIn posts (unprocessed)
â”‚   â””â”€â”€ structured_post_data.json    # Processed posts with language, tags, line count
â”‚
â”œâ”€â”€ main.py                          # Streamlit UI and frontend layout
â”œâ”€â”€ few_shot.py                      # Few-shot filtering logic (based on topic, language, length)
â”œâ”€â”€ post_generator.py                # Constructs final prompt and invokes LLM
â”œâ”€â”€ preprocessing.py                 # Metadata extraction + tag unification from raw posts
â”œâ”€â”€ LLM_helper.py                    # Groq LLaMA 3 setup via LangChain
â”‚
â”œâ”€â”€ requirements.txt                 # Project dependencies
â””â”€â”€ README.md       

# Results & Screenshots

# Contributing
Contributions are welcome! Please open an issue or submit a pull request.


# Contact / Credits
Developed by Tahajib Jakir Khan  
ðŸ“§ tahajibjakirkhan00@gmail.com 
ðŸ“Ž [LinkedIn](https://www.linkedin.com/in/tahajib-jakir-khan-53b30822b/)
